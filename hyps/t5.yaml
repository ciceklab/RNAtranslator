# T5 model details


model: "t5"    #/
# n_heads: 4  #Number of Attiontion Heads
# n_layers: 4  #Number of Layers
# n_dec_layers: 4  #Number of decoder Layers
# d_kv: 64  #
# d_ff: 2048  #
# emb_size: 512  #
seq_size: 2048  #Size of Sequences
# d_model: 512  #
# inner_size: 512  #Inner feed-forward layers Size
# dropout: 0.1  #Dropout
# vocab_size: 25128  #Vocabulary Size




# Vocabulary size of the T5 model.
vocab_size: 1000  # int, optional, defaults to 32128
# Size of the encoder layers and the pooler layer.
d_model: 512  # int, optional, defaults to 512
# Size of the key, query, value projections per attention head.
# The inner_dim of the projection layer will be defined as num_heads * d_kv.
d_kv: 64  # int, optional, defaults to 64
# Size of the intermediate feed forward layer in each T5Block.
d_ff: 1024  # int, optional, defaults to 2048
# Number of hidden layers in the Transformer encoder.
num_layers: 2  # int, optional, defaults to 6
# Number of hidden layers in the Transformer decoder.
# Will use the same value as num_layers if not set.
num_decoder_layers: null  # int, optional
# Number of attention heads for each attention layer in the Transformer encoder.
num_heads: 2  # int, optional, defaults to 8
# The number of buckets to use for each attention layer.
relative_attention_num_buckets: 32  # int, optional, defaults to 32
# The maximum distance of the longer sequences for the bucket separation.
relative_attention_max_distance: 128  # int, optional, defaults to 128
# The ratio for all dropout layers.
dropout_rate: 0.1  # float, optional, defaults to 0.1
# The dropout ratio for classifier.
classifier_dropout: 0.0  # float, optional, defaults to 0.0
# The epsilon used by the layer normalization layers.
# layer_norm_eps: 1e-6  # float, optional, defaults to 1e-6
# A factor for initializing all weight matrices.
# Should be kept to 1, used internally for initialization testing.
initializer_factor: 1  # float, optional, defaults to 1
# Type of feed forward layer to be used.
# Should be one of "relu" or "gated-gelu".
# T5v1.1 uses the "gated-gelu" feed forward projection.
# Original T5 uses "relu".
feed_forward_proj: gated-gelu  # string, optional, defaults to "relu"
# Whether or not the model should return the last key/values attentions.
# (not used by all models).
use_cache: true  # bool, optional, defaults to true