{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "books = load_dataset(\"opus_books\", \"en-fr\", split='train[:500]')\n",
    "# print(squad)\n",
    "print(books[0]['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_batch_iterator():\n",
    "    for sequence in books:\n",
    "        text = sequence['translation']\n",
    "        english, french  = text['en'], text['fr']\n",
    "        yield english\n",
    "\n",
    "def french_batch_iterator():\n",
    "    for sequence in books:\n",
    "        text = sequence['translation']\n",
    "        english, french  = text['en'], text['fr']\n",
    "        yield french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.tokenizer import BpeTokenizer\n",
    "\n",
    "english_tokenizer = BpeTokenizer(vocab_size=1000, seq_size=128)\n",
    "english_tokenizer.tokenizer.train_from_iterator(english_batch_iterator(), trainer=english_tokenizer.trainer)\n",
    "english_tokenizer.add_unk_id()\n",
    "english_tokenizer.save(\"/data6/sobhan/rllm/dataset/tokenizers\", \"en_{}_{}\".format(1000, 2048))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_tokenizer = BpeTokenizer(vocab_size=1000, seq_size=128)\n",
    "french_tokenizer.tokenizer.train_from_iterator(french_batch_iterator(), trainer=french_tokenizer.trainer)\n",
    "french_tokenizer.add_unk_id()\n",
    "french_tokenizer.save(\"/data6/sobhan/rllm/dataset/tokenizers\", \"fr_{}_{}\".format(1000, 2048))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import argparse\n",
    "import os\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import time\n",
    "import json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from src.models import get_model\n",
    "from src.utils.helpers import set_hyps\n",
    "from src.utils.tokenizer import get_tokenizer\n",
    "from src.data import get_datasets\n",
    "from train import train\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def parse_opt():\n",
    "    ################################################################ Arguments\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Multilingual RNA Implementation')\n",
    "\n",
    "    # Trainig Configuration\n",
    "    parser.add_argument('--train-data', default=\"/data6/sobhan/rllm/dataset/rpm/test_rpm.txt\", type=str, help='Fasta File Path')\n",
    "    parser.add_argument('--eval-data', default=\"/data6/sobhan/rllm/dataset/rpm/eval.txt\", type=str, help='Fasta File Path')\n",
    "    parser.add_argument('--sanity_check', default=False, type=bool, help='Sanity Check the Implementation')\n",
    "\n",
    "    parser.add_argument('--train-hyp', default=\"/data6/sobhan/rllm/hyps/train.yaml\", type=str, help='Training Arguments hyperprameters')\n",
    "    parser.add_argument('--model-hyp', default=\"/data6/sobhan/rllm/hyps/bart.yaml\", type=str, help='Model hyperprameters')\n",
    "\n",
    "    # utils\n",
    "    parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "    parser.add_argument('--results-dir', default='./results', type=str, metavar='PATH', help='path to cache (default: none)')\n",
    "\n",
    "    # args = parser.parse_args()  # running in command line\n",
    "    args = parser.parse_args('')  # running in ipynb\n",
    "\n",
    "    args.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_opt()\n",
    "print(\"============================================================================================\")\n",
    "# track total training time\n",
    "start_time = datetime.now(pytz.timezone('Turkey')).strftime(\"%Y-%m-%d %H:%M\")\n",
    "args.start_time = start_time\n",
    "\n",
    "print(\"Started training at : \", start_time)\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "# Handle Training Arguments\n",
    "args = set_hyps(args.train_hyp, args)\n",
    "args = set_hyps(args.model_hyp, args)\n",
    "args.results_dir = os.path.join(args.results_dir, args.model)\n",
    "if not os.path.exists(args.results_dir):\n",
    "    os.makedirs(args.results_dir)    \n",
    "args.results_dir = os.path.join(args.results_dir, \"run\"+str(len(os.listdir(args.results_dir)))+\"_\"+time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "os.makedirs(args.results_dir)\n",
    "\n",
    "plots_dir = args.results_dir+'/plots'\n",
    "os.mkdir(plots_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"text\", data_files=args.train_data, split=\"train[:100]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein_batch_iterator():\n",
    "    for sequence in dataset:\n",
    "        text = sequence['text']\n",
    "        protein, rna  = text.strip().split('$')\n",
    "        yield protein\n",
    "\n",
    "\n",
    "def rna_batch_iterator():\n",
    "    for sequence in dataset:\n",
    "        text = sequence['text']\n",
    "        protein, rna  = text.strip().split('$')\n",
    "        yield rna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"text\", data_files=args.train_data, split=\"train[:100]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.utils.tokenizer import BpeTokenizer\n",
    "\n",
    "protein_tokenizer = BpeTokenizer(vocab_size=1000, seq_size=128)\n",
    "protein_tokenizer.train_tokenizer(train_data=dataset)\n",
    "protein_tokenizer.save(\"/data6/sobhan/rllm/dataset/tokenizers\", \"pr_{}_{}\".format(1000, 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_tokenizer = BpeTokenizer(vocab_size=1000, seq_size=128)\n",
    "rna_tokenizer.train_tokenizer(train_data=dataset, which=False)\n",
    "rna_tokenizer.save(\"/data6/sobhan/rllm/dataset/tokenizers\", \"rna_{}_{}\".format(1000, 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dt = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = next(iter_dt)\n",
    "protein, rna  = temp['text'].strip().split('$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein\n",
    "rna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(protein_tokenizer.tokenize(protein).ids, rna_tokenizer.tokenize(rna).ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(sample, protein_tokenizer, rna_tokenizer):\n",
    "    text = sample['text']\n",
    "    protein, rna  = text.strip().split('$')\n",
    "\n",
    "    protein_tokenized = protein_tokenizer.tokenize(protein)\n",
    "    rna_tokenized = rna_tokenizer.tokenize(rna)\n",
    "    \n",
    "    # need to set these to -100 to calculate the loss properly\n",
    "    rna_labels = [-100 if i == 0 else i for i in rna_tokenized.ids]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": protein_tokenized.ids,\n",
    "        \"attention_mask\": protein_tokenized.attention_mask,\n",
    "        \"labels\": rna_labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "iterable_dataset = dataset.to_iterable_dataset()\n",
    "# Filter dataset\n",
    "# filter_protein_tokenizer = deepcopy(protein_tokenizer)\n",
    "# filter_protein_tokenizer.tokenizer.no_truncation()\n",
    "# filter_rna_tokenizer = deepcopy(rna_tokenizer)\n",
    "# filter_rna_tokenizer.tokenizer.no_truncation()\n",
    "# filtered = iterable_dataset.filter(lambda sample: (len(filter_protein_tokenizer.tokenize(sample['text'].strip().split('$')[0]).ids) <= 2048 and len(filter_rna_tokenizer.tokenize(sample['text'].strip().split('$')[1]).ids) <= 2048))\n",
    "# Shuffle dataset\n",
    "shuffled = iterable_dataset.shuffle(buffer_size = 10000)\n",
    "# Tokenize dataset\n",
    "tokenized = dataset.map(lambda sample: tokenize_dataset(sample, protein_tokenizer, rna_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dt = iter(tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = next(iter_dt)\n",
    "print(temp['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import T5Config, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "configuration = T5Config(\n",
    "                vocab_size=1000,\n",
    "                bos_token_id=1, \n",
    "                decoder_start_token_id=0)\n",
    "model = AutoModelForSeq2SeqLM.from_config(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import get_t5_model\n",
    "model = get_t5_model(args)\n",
    "\n",
    "args.model_size = sum(p.numel() for p in model.parameters())\n",
    "print(\"Model Size: \", sum(p.numel() for p in model.parameters()))\n",
    "print(model)\n",
    "\n",
    "# Saving the configs\n",
    "args_dict = vars(args)\n",
    "with open(args.results_dir + '/Main Config.json', 'w') as json_file:\n",
    "    json.dump(args_dict, json_file, indent=4)\n",
    "print(\"Config saved to \", args.results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"prot\"\n",
    "target_lang = \"rna\"\n",
    "from transformers import TrainingArguments, Trainer\n",
    "batch_size = 16\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "train_args = TrainingArguments(\n",
    "    f\"train-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    max_steps=4000,\n",
    "    # predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    do_eval=True,\n",
    "    # resume_from_checkpoint='/data6/sobhan/rllm/train-prot-to-rna/checkpoint-3000'\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=tokenized,\n",
    "    eval_dataset=tokenized,\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.from_pretrained(\"/data6/sobhan/rllm/train2-prot-to-rna/checkpoint-4000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = iter(tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = next(temp_data)\n",
    "protein_tokenizer.decode(temp['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_tokenizer.decode(temp['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(torch.tensor(temp['input_ids']).unsqueeze(0).to(model.device), max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_tokenizer.decode(model.generate(torch.tensor(temp['input_ids']).unsqueeze(0).to(model.device))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tokenize_dataset(books[1], english_tokenizer=english_tokenizer, french_tokenizer=french_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp['labels']), len(temp['decoder_input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = books.map(lambda sample: tokenize_dataset(sample, english_tokenizer, french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp = next(iter(tokenized))\n",
    "len(data_temp[\"input_ids\"]),len(data_temp[\"decoder_input_ids\"]),len(data_temp[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(args=args)\n",
    "\n",
    "args.model_size = sum(p.numel() for p in model.parameters())\n",
    "print(\"Model Size: \", sum(p.numel() for p in model.parameters()))\n",
    "print(model)\n",
    "\n",
    "# Saving the configs\n",
    "args_dict = vars(args)\n",
    "with open(args.results_dir + '/Main Config.json', 'w') as json_file:\n",
    "    json.dump(args_dict, json_file, indent=4)\n",
    "print(\"Config saved to \", args.results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "# args.device = 'cuda:0'\n",
    "# model.device = 'cuda:0'\n",
    "\n",
    "if not args.sanity_check:\n",
    "    train(args=args, wandb=wandb, model=model, train_dataset=tokenized, eval_dataset=tokenized, enc_tokenizer=english_tokenizer, dec_tokenizer=french_tokenizer)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "end_time = datetime.now(pytz.timezone('Turkey')).strftime(\"%Y-%m-%d %H:%M\")\n",
    "print(\"Finished training at : \", end_time)\n",
    "print(\"============================================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AAaaaaaaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/anaconda3/envs/rllm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
