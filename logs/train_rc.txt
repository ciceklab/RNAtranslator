Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
enable_padding(max_length=X) is deprecated, use enable_padding(length=X) instead
enable_padding(max_length=X) is deprecated, use enable_padding(length=X) instead
Number of GPUs available: 1
GPU 0 is: NVIDIA TITAN RTX
GPU 0 ID: cuda:0
Using device: cuda
cuda
============================================================================================
Started training at :  2024-11-02 00:16
============================================================================================
Map:   0%|          | 0/392 [00:00<?, ? examples/s]Map:  43%|████▎     | 170/392 [00:00<00:00, 1682.75 examples/s]Map:  98%|█████████▊| 383/392 [00:00<00:00, 1941.54 examples/s]Map: 100%|██████████| 392/392 [00:00<00:00, 1166.90 examples/s]
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Model Size:  8126208
T5ForConditionalGeneration(
  (shared): Embedding(1000, 256)
  (encoder): T5Stack(
    (embed_tokens): Embedding(1000, 256)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=256, out_features=128, bias=False)
              (k): Linear(in_features=256, out_features=128, bias=False)
              (v): Linear(in_features=256, out_features=128, bias=False)
              (o): Linear(in_features=128, out_features=256, bias=False)
              (relative_attention_bias): Embedding(32, 4)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedActDense(
              (wi_0): Linear(in_features=256, out_features=1024, bias=False)
              (wi_1): Linear(in_features=256, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=256, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1-3): 3 x T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=256, out_features=128, bias=False)
              (k): Linear(in_features=256, out_features=128, bias=False)
              (v): Linear(in_features=256, out_features=128, bias=False)
              (o): Linear(in_features=128, out_features=256, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedActDense(
              (wi_0): Linear(in_features=256, out_features=1024, bias=False)
              (wi_1): Linear(in_features=256, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=256, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): T5Stack(
    (embed_tokens): Embedding(1000, 256)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=256, out_features=128, bias=False)
              (k): Linear(in_features=256, out_features=128, bias=False)
              (v): Linear(in_features=256, out_features=128, bias=False)
              (o): Linear(in_features=128, out_features=256, bias=False)
              (relative_attention_bias): Embedding(32, 4)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=256, out_features=128, bias=False)
              (k): Linear(in_features=256, out_features=128, bias=False)
              (v): Linear(in_features=256, out_features=128, bias=False)
              (o): Linear(in_features=128, out_features=256, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedActDense(
              (wi_0): Linear(in_features=256, out_features=1024, bias=False)
              (wi_1): Linear(in_features=256, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=256, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1-3): 3 x T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=256, out_features=128, bias=False)
              (k): Linear(in_features=256, out_features=128, bias=False)
              (v): Linear(in_features=256, out_features=128, bias=False)
              (o): Linear(in_features=128, out_features=256, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=256, out_features=128, bias=False)
              (k): Linear(in_features=256, out_features=128, bias=False)
              (v): Linear(in_features=256, out_features=128, bias=False)
              (o): Linear(in_features=128, out_features=256, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedActDense(
              (wi_0): Linear(in_features=256, out_features=1024, bias=False)
              (wi_1): Linear(in_features=256, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=256, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (lm_head): Linear(in_features=256, out_features=1000, bias=False)
)
Config saved to  /data6/helya/RLLM/results/train/t5/run28_20241101-211625
train
TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)
  0%|          | 0/1000000 [00:00<?, ?it/s]  0%|          | 1/1000000 [00:03<1063:28:53,  3.83s/it]                                                          0%|          | 1/1000000 [00:03<1063:28:53,  3.83s/it]  0%|          | 2/1000000 [00:06<952:32:43,  3.43s/it]   0%|          | 3/1000000 [00:10<916:37:34,  3.30s/it]  0%|          | 4/1000000 [00:13<900:34:27,  3.24s/it]  0%|          | 5/1000000 [00:16<891:03:15,  3.21s/it]  0%|          | 6/1000000 [00:19<885:52:49,  3.19s/it]  0%|          | 7/1000000 [00:22<881:32:00,  3.17s/it]  0%|          | 8/1000000 [00:25<879:49:49,  3.17s/it]  0%|          | 9/1000000 [00:29<878:39:38,  3.16s/it]  0%|          | 10/1000000 [00:32<877:58:14,  3.16s/it]  0%|          | 11/1000000 [00:35<877:09:12,  3.16s/it]  0%|          | 12/1000000 [00:38<876:38:47,  3.16s/it]  0%|          | 13/1000000 [00:41<876:30:41,  3.16s/it]  0%|          | 14/1000000 [00:44<877:19:59,  3.16s/it]  0%|          | 15/1000000 [00:47<877:29:03,  3.16s/it]  0%|          | 16/1000000 [00:51<878:21:49,  3.16s/it]  0%|          | 17/1000000 [00:54<878:28:38,  3.16s/it]  0%|          | 18/1000000 [00:57<879:11:05,  3.17s/it]  0%|          | 19/1000000 [01:00<879:01:29,  3.16s/it]  0%|          | 20/1000000 [01:03<878:39:09,  3.16s/it]  0%|          | 21/1000000 [01:06<878:44:20,  3.16s/it]  0%|          | 22/1000000 [01:10<878:57:41,  3.16s/it]  0%|          | 23/1000000 [01:13<880:13:28,  3.17s/it]  0%|          | 24/1000000 [01:16<880:22:39,  3.17s/it]  0%|          | 25/1000000 [01:19<880:39:34,  3.17s/it]  0%|          | 26/1000000 [01:22<880:20:02,  3.17s/it]  0%|          | 27/1000000 [01:25<879:49:43,  3.17s/it]  0%|          | 28/1000000 [01:29<878:49:25,  3.16s/it]  0%|          | 29/1000000 [01:32<878:32:14,  3.16s/it]  0%|          | 30/1000000 [01:35<879:22:47,  3.17s/it]  0%|          | 31/1000000 [01:38<879:41:13,  3.17s/it]  0%|          | 32/1000000 [01:41<879:52:28,  3.17s/it]  0%|          | 33/1000000 [01:44<880:49:29,  3.17s/it]  0%|          | 34/1000000 [01:48<881:12:37,  3.17s/it]