Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/var/anaconda3/envs/rllm/lib/python3.12/site-packages/datasets/table.py:1392: FutureWarning: promote has been superseded by mode='default'.
  table = cls._concat_blocks(blocks, axis=0)
enable_padding(max_length=X) is deprecated, use enable_padding(length=X) instead



enable_padding(max_length=X) is deprecated, use enable_padding(length=X) instead



Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Number of GPUs available: 8
GPU 0 is: NVIDIA TITAN RTX
GPU 0 ID: cuda:0
GPU 1 is: NVIDIA TITAN RTX
GPU 1 ID: cuda:1
GPU 2 is: NVIDIA GeForce RTX 2080 Ti
GPU 2 ID: cuda:2
GPU 3 is: NVIDIA GeForce RTX 2080 Ti
GPU 3 ID: cuda:3
GPU 4 is: NVIDIA GeForce RTX 2080 Ti
GPU 4 ID: cuda:4
GPU 5 is: NVIDIA GeForce RTX 2080 Ti
GPU 5 ID: cuda:5
GPU 6 is: NVIDIA GeForce RTX 2080 Ti
GPU 6 ID: cuda:6
GPU 7 is: NVIDIA GeForce RTX 2080 Ti
GPU 7 ID: cuda:7
Using device: cuda
cuda
============================================================================================
Started training at :  2024-08-08 18:50
============================================================================================
Model Size:  41423616
T5ForConditionalGeneration(
  (shared): Embedding(1000, 512)
  (encoder): T5Stack(
    (embed_tokens): Embedding(1000, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=768, bias=False)
              (k): Linear(in_features=512, out_features=768, bias=False)
              (v): Linear(in_features=512, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 12)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseActDense(
              (wi): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1-5): 5 x T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=768, bias=False)
              (k): Linear(in_features=512, out_features=768, bias=False)
              (v): Linear(in_features=512, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseActDense(
              (wi): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): T5Stack(
    (embed_tokens): Embedding(1000, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=768, bias=False)
              (k): Linear(in_features=512, out_features=768, bias=False)
              (v): Linear(in_features=512, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 12)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=768, bias=False)
              (k): Linear(in_features=512, out_features=768, bias=False)
              (v): Linear(in_features=512, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseActDense(
              (wi): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1-5): 5 x T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=768, bias=False)
              (k): Linear(in_features=512, out_features=768, bias=False)
              (v): Linear(in_features=512, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=768, bias=False)
              (k): Linear(in_features=512, out_features=768, bias=False)
              (v): Linear(in_features=512, out_features=768, bias=False)
              (o): Linear(in_features=768, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseActDense(
              (wi): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (lm_head): Linear(in_features=512, out_features=1000, bias=False)
)
Config saved to  /data6/sobhan/rllm/results/train/t5/run20_20240808-155056
Traceback (most recent call last):
  File "/data6/sobhan/rllm/main.py", line 148, in <module>
    main(args, wandb)
  File "/data6/sobhan/rllm/main.py", line 124, in main
    train(args=args, wandb=wandb, model=model, train_dataset=train_dataset, eval_dataset=eval_dataset, enc_tokenizer=protein_tokenizer, dec_tokenizer=rna_tokenizer)
  File "/data6/sobhan/rllm/train.py", line 116, in train
    trainer = Trainer(
              ^^^^^^^^
  File "/var/anaconda3/envs/rllm/lib/python3.12/site-packages/transformers/trainer.py", line 480, in __init__
    raise ValueError("Using fsdp only works in distributed training.")
ValueError: Using fsdp only works in distributed training.
