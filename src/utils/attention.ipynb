{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/anaconda3/envs/rllm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "from tokenizer import BpeTokenizer\n",
    "# from helpers import preprocess_rna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config for model and training\n",
    "results_dir = Path(\"attentions\")\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "# Device setup\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "    \n",
    "# Load the T5 model\n",
    "model_path = \"/data6/sobhan/rllm/results/train/t5/run3_20240822-152114/checkpoints/checkpoint-349800\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_rna(rna):\n",
    "    return rna.lower().replace(\n",
    "                    'a', 'B').replace('c', 'J').replace('u', 'U').replace('g', 'Z')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"max_length\": 100,\n",
    "    \"num_beams\": 3,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 3,\n",
    "    \"top_p\": 1.0,\n",
    "    \"repetition_penalty\": 1.0\n",
    "}\n",
    "\n",
    "# Input protein sequence\n",
    "protein_sequence =  'MSNGYEDHMAEDCRGDIGRTNLIVNYLPQNMTQDELRSLFSSIGEVESAKLIRDKVAGHSLGYGFVNYVTAKDAERAINTLNGLRLQSKTIKVSYARPSSEVIKDANLYISGLPRTMTQKDVEDMFSRFGRIINSRVLVDQTTGLSRGVAFIRFDKRSEAEEAITSFNGHKPPGSSEPITVKFAANPNQNKNVALLSQLYHSPARRFGGPVHHQAQRFRFSPMGVDHMSGLSGVNVPGNASSGWCIFIYNLGQDADEGILWQMFGPFGAVTNVKVIRDFNTNKCKGFGFVTMTNYEEAAMAIASLNGYRLGDKILQVSFKTNKSHK'\n",
    "rna_sequence = 'ttgggcaaaagacttttaaccctcccgaaccttggctccttacctaaaaaatagagcatctctaaagtctcttataaatgtaaatttccataaAUUAUUU'\n",
    "rna_sequence = preprocess_rna(rna_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enable_padding(max_length=X) is deprecated, use enable_padding(length=X) instead\n",
      "enable_padding(max_length=X) is deprecated, use enable_padding(length=X) instead\n"
     ]
    }
   ],
   "source": [
    "rna_tokenizer = BpeTokenizer(vocab_size=1000, seq_size=1024)\n",
    "rna_tokenizer.load(\"/data6/sobhan/RLLM/dataset/tokenizers/bpe_rna_1000_1024.json\")\n",
    "\n",
    "protein_tokenizer = BpeTokenizer(vocab_size=1000, seq_size=1024)\n",
    "protein_tokenizer.load(\"/data6/sobhan/RLLM/dataset/tokenizers/bpe_protein_1000_1024.json\")\n",
    "\n",
    "# Tokenize the sequences\n",
    "input_ids = protein_tokenizer.tokenize(protein_sequence)\n",
    "inputs = torch.tensor(input_ids.ids, dtype=torch.long).to(device)\n",
    "inputs = inputs.unsqueeze(0)\n",
    "\n",
    "output_ids = rna_tokenizer.tokenize(rna_sequence)\n",
    "output = torch.tensor(output_ids.ids, dtype=torch.long).to(device)\n",
    "output = output.unsqueeze(0)\n",
    "\n",
    "# Pass through the model with attention outputs enabled\n",
    "out = model(input_ids=inputs, decoder_input_ids=output, return_dict=True, output_attentions=True)\n",
    "\n",
    "# Extract attentions\n",
    "encoder_attentions = out.encoder_attentions\n",
    "cross_attentions  = out.cross_attentions\n",
    "decoder_attentions = out.decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 0, 15, 5, 1, 2, 3, 6, 4, 7, 12, 9, 26, 63, 30, 27, 23, 28, 24, 64, 62, 59, 25, 65, 66, 48, 45, 10, 67, 43]\n",
      "z z z j b b b b z b j b jjjjzb bjjzzj j j j b b b b b b b b j b j b b b b b b b b b u u u\n",
      "{t}{t}{Z}{Z}{Z}{J}{B}{B}{B}{B}{Z}B{J}tt{t}{t}{B}{B}J{J}J{t}{J}{J}{J}{Z}{B}{B}J{J}t{t}{Z}{Z}Jt{J}{J}ttBJ{J}{t}{B}B{B}{B}{B}BtBZBZJB{t}{J}{t}{J}{t}{B}{B}{B}{Z}{t}{J}{t}{J}{t}{t}{B}{t}{B}{B}{B}{t}{Z}{t}{B}{B}{B}{t}{t}{t}{J}{J}{B}{t}{B}{B}{B}{U}{U}{B}{U}{U}{U}\n",
      "The most-attended tokens overlap with the binding site.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def analyze_attention(attention_tensor, top_n=100):\n",
    "    avg_attention = attention_tensor.mean(dim=1)  # Shape: [1, 1024, 1024]\n",
    "    attention_per_token = avg_attention.sum(dim=1).squeeze()  # Shape: [1024]\n",
    "    top_indices = torch.argsort(attention_per_token, descending=True)[:top_n]\n",
    "    top_indices_list = np.sort(top_indices.tolist())\n",
    "\n",
    "    return top_indices_list, attention_per_token[top_indices].tolist(), top_indices.tolist()\n",
    "\n",
    "def highlight_attention(protein_sequence, attention_indices):\n",
    "    highlighted_sequence = \"\"\n",
    "    for i, char in enumerate(protein_sequence):\n",
    "        if i in attention_indices:\n",
    "            highlighted_sequence += f\"{{{char}}}\"\n",
    "        else:\n",
    "            highlighted_sequence += char\n",
    "    return highlighted_sequence\n",
    "\n",
    "def does_binding_site_overlap(attended_tokens, binding_start, binding_end, token_to_nt_mapping):\n",
    "    for token_idx in attended_tokens:\n",
    "        token_range = token_to_nt_mapping[token_idx]\n",
    "        if not (token_range[1] < binding_start or token_range[0] > binding_end):\n",
    "            return True\n",
    "    return False \n",
    "\n",
    "\n",
    "dec_attns = decoder_attentions[-1]\n",
    "print(analyze_attention(dec_attns, 30)[-1])\n",
    "print(rna_tokenizer.decode(np.array(output_ids.ids)[analyze_attention(dec_attns)[0]]))\n",
    "print(highlight_attention(rna_sequence, analyze_attention(dec_attns)[0]))\n",
    "\n",
    "_, _, attended_tokens = analyze_attention(dec_attns, 150)\n",
    "\n",
    "vocab = rna_tokenizer.tokenizer.get_vocab()  \n",
    "id2token = {v: k for k, v in vocab.items()}\n",
    "def convert_ids_to_tokens(token_ids):\n",
    "    return [id2token[id_] for id_ in token_ids]\n",
    "decoded_tokens = convert_ids_to_tokens(output_ids.ids)  \n",
    "\n",
    "# Build a mapping: for each token, which substring positions in the decoded RNA does it cover?\n",
    "token_to_nt_mapping = []\n",
    "start_pos = 0\n",
    "for tok in decoded_tokens:\n",
    "    # 'tok' might be multiple characters, e.g. \"ACG\" if BPE merges them\n",
    "    token_len = len(tok)\n",
    "    token_to_nt_mapping.append((start_pos, start_pos + token_len - 1))\n",
    "    start_pos += token_len\n",
    "\n",
    "\n",
    "overlap = does_binding_site_overlap(attended_tokens, 1, 21, token_to_nt_mapping)\n",
    "\n",
    "if overlap:\n",
    "    print(\"The most-attended tokens overlap with the binding site.\")\n",
    "else:\n",
    "    print(\"The most-attended tokens do not overlap with the binding site.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binding site coverage by top-attended tokens: 90.48%\n"
     ]
    }
   ],
   "source": [
    "def compute_binding_site_coverage(top_attended_tokens, token_to_nt_mapping, binding_start, binding_end):\n",
    "    # The total length of the binding site in nucleotides\n",
    "    binding_length = binding_end - binding_start + 1\n",
    "    \n",
    "    # Track how many nucleotides of that binding site are covered by top-attended tokens\n",
    "    overlap_count = 0\n",
    "    \n",
    "    for token_idx in top_attended_tokens:\n",
    "        token_start, token_end = token_to_nt_mapping[token_idx]\n",
    "        \n",
    "        overlap_start = max(token_start, binding_start)\n",
    "        overlap_end = min(token_end, binding_end)\n",
    "        \n",
    "        if overlap_start <= overlap_end:\n",
    "            overlap_count += (overlap_end - overlap_start + 1)\n",
    "    coverage_percentage = 100.0 * overlap_count / binding_length\n",
    "    return coverage_percentage\n",
    "\n",
    "coverage = compute_binding_site_coverage(\n",
    "    attended_tokens, \n",
    "    token_to_nt_mapping, \n",
    "    binding_start=20, \n",
    "    binding_end=40\n",
    ")\n",
    "print(f\"Binding site coverage by top-attended tokens: {coverage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
